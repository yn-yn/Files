


> Written with [StackEdit中文版](https://stackedit.cn/).

# Stereo Vision-based Fast Obstacles Avoidance without Obstacles Discrimination for Indoor UAVs (基于立体视觉的室内无人机快速躲避障碍物而不进行障碍物识别的研究)

> 参考文献: [Stereo Vision-based Fast Obstacles Avoidance without Obstacles Discrimination for Indoor UAVs](https://ieeexplore.ieee.org/abstract/document/6010062)(基于立体视觉的室内无人机快速躲避障碍物而不进行障碍物识别的研究 )

目前国内外的研究中运用较多的导航方式有**超声波导航、激光导航、视觉导航**。和激光雷达、超声波相比，**计算机视觉具有以下几个优点**：
1. 即使在丢弃了绝大部介的视觉信息后，所乘下的关于周舞环境的信息仍然比激光吉达和超声波更多更**精确**
2. 激光雷达和超声波的原理都是通过主动发射脉冲和接受反射脉冲来测距的，当飞行器处于复杂环境时容易受到干扰，而视觉测距的方法是被动测量，因此**外界干扰可以减少到最小**
3. 激光雷达和超声波数据的采样周期一般比摄像机长，因此不能**及时**对高速运动的飞行器提供信息，做出路径规划以及轨迹控制

## 环境感知

传统的环境感知方法通常都是识别障碍物轮廓，逐个分离障碍物，在避障算法中再进行障碍物信息综合。对于位于地面上的障碍物，Nedevsch刀以及 Iemonde 和 Devy 在论文中通过像素点的高度判断从视差图中去除地面像素点，剩下的点被认为是障碍物。Dang 和 HofinanT 使用区域生长算法分离视差图上相近视差区域，并且判断该区域中像素的多少决定是否是障碍物。 Claudio Caraff, Stefano Cattani和 Paolo Grislerir101通过引入一系列滤波器对视差图进行处理。

但是在复杂场景中，尤其是无人机在室内飞行的过程中，所有探测到的物体都是需要回避的障碍物，逐个分离这些障碍物的方法几乎是不可实现的，而且也是没有必要的。分析当人类进入到一个复杂环境中快速避障的过程，很容易发现我们人类在决策过程中并非一定要逐个分析每个障碍物信息，更多情况下，我们是直接寻找一个安全出口快速避障的。视觉感知的内容本身就是障碍物信息综合的结果。成像过程中，图像反映了所有可视障碍物的轮廓、位置及遮挡等因素综合之后的最终结果，通过每一个象素点记录下来。通过立体视觉系统获取场景深度图，得到每一个象素点对应的实际空间点位置信息，就可以全面的记录环境信息，为回避决策提供依据。

场景深度图获取可通过图1所示六个步骤完成。
![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/09/21/q8908cE58UY0Li1t.png)

## 双目立体匹配算法

目的: 在同一场景的两幅图像中寻找匹配点,求取视差图,并根据三角测量原理匹配点深度。

研究目标: 在复杂场景中,提高算法的去歧义匹配和抗干扰能力,降低现实的复杂程度和计算量

基于边缘信息的区域立体匹配方法具体步骤: 
![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/uU7rtTM8Yi6wKznE.png =500x)

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/4FkARpj97wfuqNZd.png =500x)
![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/IjNECLbRS6S1NxuY.png =200x)

## 计算深度

经过图像对的立体校正将一般的双目立体视觉成像模型中的左右图像平面重投影后得到如图5所示的理想的成像模型。其坐标原点为右摄像头投影中心，X轴由原点指向右，Z轴垂直于摄像机成像平面指向前方，Y轴垂直于X-Z平面箭头向下。

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/sUuw4RWURXGZ8ihL.png =300x)

已知立体成像模型和兀配视差后，三维距离的恢复是很容易的。由立体成像得到视差图后，如果给定屏幕坐标和摄像机内参数矩阵，图像坐标即可以重投影到三维中，重投影矩阵如式2所示:

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/nDa9CH185dKkAP4m.png =400x)

这里，除Cx' 外的所有参数都来自于右图像，Cx' 是主点在左图像上的 X 坐标。如果主光线在无穷远处相交，那么Cx=Cx' ，所以右下角的项可近似为 0。给定一个二维齐次点和其关联的视差d，可以将此点投影到三维:

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/XritD2IRxFT365nv.png =300x)

三维坐标就是(X/W, Y/W, Z/W)。

运用式(3)，可以计算出每一个像素点的空间坐标，得到场景深度信息。用灰度图表示的场景深度信息如图6表示。

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/jey0Ek1q2kuXYv4r.png =300x)

## 景深描述

文章采用基于栅格的景深描述。

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/AqWvZ130dO8Rt5Yf.png =500x)

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/pPwCbhcmrIgYMSmp.png =500x)

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/YnT9IpPBlArCzr4E.png =500x)

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/T82CkuUgxIPISVuB.png =500x)

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/N4GxLUWHN6dEKR6m.png =500x)

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/tXU4ufhEsvecwz4O.png =500x)

## 无人机飞行路径规划

在本文研究中，回避路径规划分为两部分: 第一部分，生成当前飞行阶段的最优引导点，作为航迹规划的虚拟目标点; 第二部分，在该虚拟目标点的基础上，无人机完成当前阶段的回避路径规划。

首先，根据实际飞机飞行环境及飞行速度，本文规定1m为安全深度值，在飞机的正前方该距离范围之内一旦出现障碍物，飞机将有可能在反应时间内发生碰撞；2m为飞行步长深度，飞机在每一次回避行为前行(即Z轴方向位移)一个步长；3m为决策深度，飞机的回避引导信息由决策距离处的安全地图给出；5m为最大感知深度，飞机有效感知范围为5m，一律大于该景深的点均当该景深处理。

将图7所示的可感知空间Space划分为三个部分如图12所示：左边黄色部分表示行为区，深度范围为0至步长深度，飞机的回避行为发生在该区内；右边深蓝色部分表示决策区，深度范围为决策深度至最大感知深度，飞机的回避路径根据该区域中障碍物信息生成；中间浅蓝色部分表示预留区，位于行为区与决策区之间，厚度为安全深度值。

![](https://raw.githubusercontent.com/yn-yn/image1/master/2022/10/19/W5Izih1Fhq1NEa1K.png =250x)

具体路径规划过程在此处省略。

## 总结

文章突破传统的将障碍物分离的环境感知算法，**直接采用场景深度图作为障碍物描述**，更适用于障碍物众多的复杂环境。其中，在视差图获取过程中的立体匹配环节，采用**基于边缘信息的区域立体匹配方法**，直接对匹配正确率高的边缘区域进行区域立体匹配，直接消除不必要的计算量，大大提高了立体匹配速度。采用**基于栅格的景深图描述法**，而将划分出的每一个栅格都用相应的景深标记，便于快速绘制安全地图，迅速确定虚拟目标点，逐步增加航路点，规划出安全通道。仿真结果表明，该方法**在典型的结构化的障碍物众多的室内环境中有很好的效果**。中频率为2830MHz的双核处理器.4096MB DDRII内存的戴尔 OptiPlex960系列电脑进行一次亚象素精度的环境感知、描述及路径规划总共耗时0.35s，满足系统实时性要求。


<!--stackedit_data:
eyJoaXN0b3J5IjpbMjQ3NDkzOTg4XX0=
-->